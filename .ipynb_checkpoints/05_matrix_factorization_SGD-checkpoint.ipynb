{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d56501-4880-438f-a63d-ba800dab5d0e",
   "metadata": {},
   "source": [
    "#### Objective:\n",
    "This script implements a **matrix factorization approach** using **stochastic gradient descent (SGD)** to recommend movies based on user-item interaction data. The user-item matrix is decomposed into latent factors, with bias terms for users and items, to predict missing ratings.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "\n",
    "1. **User-Item Matrix**:\n",
    "   Construct the user-item matrix $R \\in \\mathbb{R}^{m \\times n}$, where:\n",
    "   - $m$: Number of users,\n",
    "   - $n$: Number of items,\n",
    "   - $R_{ui}$: Rating given by user $u$ to item $i$ (or $0$ if not rated).\n",
    "\n",
    "2. **Latent Factor Decomposition**:\n",
    "   Decompose $R$ into:\n",
    "   $R_{ui} \\approx \\hat{R}_{ui} = \\mu + b_u + b_i + P_u \\cdot Q_i^\\top$.\n",
    "   Where:\n",
    "   - $\\mu$: Global mean rating,\n",
    "   - $b_u$: Bias for user $u$,\n",
    "   - $b_i$: Bias for item $i$,\n",
    "   - $P_u$: Latent factors for user $u$,\n",
    "   - $Q_i$: Latent factors for item $i$,\n",
    "   - $k$: Number of latent dimensions (hyperparameter).\n",
    "\n",
    "3. **Optimization**:\n",
    "   Minimize the loss function:\n",
    "   $\\mathcal{L} = \\sum_{(u, i) \\in \\mathcal{K}} \\left( R_{ui} - \\hat{R}_{ui} \\right)^2 + \\lambda \\left( \\|P\\|_F^2 + \\|Q\\|_F^2 + \\|b_u\\|_2^2 + \\|b_i\\|_2^2 \\right)$.\n",
    "   Where:\n",
    "   - $\\mathcal{K}$: Set of observed ratings,\n",
    "   - $\\lambda$: Regularization parameter,\n",
    "   - $\\| \\cdot \\|_F$: Frobenius norm for latent factors,\n",
    "   - $\\| \\cdot \\|_2$: L2 norm for biases.\n",
    "\n",
    "4. **Stochastic Gradient Descent**:\n",
    "   Update parameters iteratively for each observed rating $(u, i)$:\n",
    "   - Biases:\n",
    "     $b_u \\gets b_u + \\gamma \\left( e_{ui} - \\lambda b_u \\right), \\quad b_i \\gets b_i + \\gamma \\left( e_{ui} - \\lambda b_i \\right)$,\n",
    "   - Latent Factors:\n",
    "     $P_u \\gets P_u + \\gamma \\left( e_{ui} Q_i - \\lambda P_u \\right), \\quad Q_i \\gets Q_i + \\gamma \\left( e_{ui} P_u - \\lambda Q_i \\right)$.\n",
    "   Where:\n",
    "   - $e_{ui} = R_{ui} - \\hat{R}_{ui}$,\n",
    "   - $\\gamma$: Learning rate.\n",
    "\n",
    "5. **Predicted Ratings**:\n",
    "   After training, predict ratings as:\n",
    "   $\\hat{R}_{ui} = \\mu + b_u + b_i + P_u \\cdot Q_i^\\top$.\n",
    "\n",
    "6. **Recommendations**:\n",
    "   For the target user $u$:\n",
    "   - Predict ratings for all items: $\\hat{R}_{u, :}$,\n",
    "   - Exclude items already rated by $u$,\n",
    "   - Rank remaining items by predicted score and return the top $N$ recommendations.\n",
    "\n",
    "#### Hyperparameters:\n",
    "- `latent_factors (k)`: Number of latent dimensions.\n",
    "- `learning_rate (\\gamma)`: Step size for gradient updates.\n",
    "- `regularization (\\lambda)`: Penalization for large latent factor values.\n",
    "- `max_iterations`: Number of optimization steps.\n",
    "- `target_user_id`: User ID for recommendations.\n",
    "- `top_n_recommendations`: Number of recommendations to display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ded215-8ca8-4038-90cf-5b6af3ad8cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Data Tables:\n",
      "Movies Table: 9742 rows, 3 columns\n",
      "Ratings Table: 100836 rows, 4 columns\n",
      "\n",
      "Movies Table (Sample):\n",
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n",
      "\n",
      "Ratings Table (Sample):\n",
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n",
      "\n",
      "Step 1: Creating User-Item Matrix...\n",
      "User-Item Matrix (First 5 Users, First 5 Movies):\n",
      "movieId    1    2    3    4    5\n",
      "userId                          \n",
      "1        4.0  0.0  4.0  0.0  0.0\n",
      "2        0.0  0.0  0.0  0.0  0.0\n",
      "3        0.0  0.0  0.0  0.0  0.0\n",
      "4        0.0  0.0  0.0  0.0  0.0\n",
      "5        4.0  0.0  0.0  0.0  0.0\n",
      "\n",
      "Step 2: Initializing Parameters...\n",
      "Number of Users: 610, Number of Items: 9724\n",
      "Global Mean Rating: 3.50\n",
      "\n",
      "Step 3: Training Matrix Factorization Model...\n",
      "Iteration 1/50, Loss: 84168.26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters and Variables\n",
    "path = 'data/ml-latest-small/'  # Path to dataset\n",
    "movies_path = path + \"movies.csv\"\n",
    "ratings_path = path + \"ratings.csv\"\n",
    "latent_factors = 10  # Number of latent factors (k)\n",
    "learning_rate = 0.01  # Learning rate for SGD\n",
    "regularization = 0.1  # Regularization parameter (lambda)\n",
    "max_iterations = 50  # Number of iterations for SGD\n",
    "target_user_id = 1  # ID of the target user for recommendations\n",
    "\n",
    "# Load datasets\n",
    "movies = pd.read_csv(movies_path)\n",
    "ratings = pd.read_csv(ratings_path)\n",
    "\n",
    "# Describe base data tables\n",
    "print(\"\\nBase Data Tables:\")\n",
    "print(f\"Movies Table: {movies.shape[0]} rows, {movies.shape[1]} columns\")\n",
    "print(f\"Ratings Table: {ratings.shape[0]} rows, {ratings.shape[1]} columns\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nMovies Table (Sample):\")\n",
    "print(movies.head(5))\n",
    "print(\"\\nRatings Table (Sample):\")\n",
    "print(ratings.head(5))\n",
    "\n",
    "# Step 1: Create User-Item Matrix\n",
    "print(\"\\nStep 1: Creating User-Item Matrix...\")\n",
    "user_item_matrix = ratings.pivot(index=\"userId\", columns=\"movieId\", values=\"rating\").fillna(0)\n",
    "print(\"User-Item Matrix (First 5 Users, First 5 Movies):\")\n",
    "print(user_item_matrix.iloc[:5, :5])\n",
    "\n",
    "# Step 2: Initialize Parameters\n",
    "print(\"\\nStep 2: Initializing Parameters...\")\n",
    "num_users, num_items = user_item_matrix.shape\n",
    "P = np.random.normal(scale=1./latent_factors, size=(num_users, latent_factors))  # User latent factors\n",
    "Q = np.random.normal(scale=1./latent_factors, size=(num_items, latent_factors))  # Item latent factors\n",
    "b_u = np.zeros(num_users)  # User biases\n",
    "b_i = np.zeros(num_items)  # Item biases\n",
    "global_mean = user_item_matrix.values[user_item_matrix.values > 0].mean()  # Global mean rating\n",
    "\n",
    "print(f\"Number of Users: {num_users}, Number of Items: {num_items}\")\n",
    "print(f\"Global Mean Rating: {global_mean:.2f}\")\n",
    "\n",
    "# Step 3: Train Matrix Factorization Model\n",
    "print(\"\\nStep 3: Training Matrix Factorization Model...\")\n",
    "for iteration in range(max_iterations):\n",
    "    for user in range(num_users):\n",
    "        for item in range(num_items):\n",
    "            rating = user_item_matrix.values[user, item]\n",
    "            if rating > 0:  # Only consider observed ratings\n",
    "                # Compute prediction and error\n",
    "                prediction = global_mean + b_u[user] + b_i[item] + np.dot(P[user, :], Q[item, :])\n",
    "                error = rating - prediction\n",
    "\n",
    "                # Update biases\n",
    "                b_u[user] += learning_rate * (error - regularization * b_u[user])\n",
    "                b_i[item] += learning_rate * (error - regularization * b_i[item])\n",
    "\n",
    "                # Update latent factors\n",
    "                P[user, :] += learning_rate * (error * Q[item, :] - regularization * P[user, :])\n",
    "                Q[item, :] += learning_rate * (error * P[user, :] - regularization * Q[item, :])\n",
    "\n",
    "    # Compute training loss\n",
    "    loss = 0\n",
    "    for user in range(num_users):\n",
    "        for item in range(num_items):\n",
    "            rating = user_item_matrix.values[user, item]\n",
    "            if rating > 0:\n",
    "                prediction = global_mean + b_u[user] + b_i[item] + np.dot(P[user, :], Q[item, :])\n",
    "                loss += (rating - prediction) ** 2\n",
    "    loss += regularization * (np.sum(b_u ** 2) + np.sum(b_i ** 2) + np.sum(P ** 2) + np.sum(Q ** 2))\n",
    "    print(f\"Iteration {iteration + 1}/{max_iterations}, Loss: {loss:.2f}\")\n",
    "\n",
    "# Step 4: Generate Recommendations\n",
    "print(\"\\nStep 4: Generating Recommendations...\")\n",
    "user_index = target_user_id - 1  # Adjust for zero indexing\n",
    "predictions = global_mean + b_u[user_index] + b_i + np.dot(P[user_index, :], Q.T)\n",
    "rated_items = user_item_matrix.columns[user_item_matrix.values[user_index, :] > 0]\n",
    "unrated_items = [item for item in user_item_matrix.columns if item not in rated_items]\n",
    "recommendations = pd.DataFrame({\n",
    "    \"movieId\": unrated_items,\n",
    "    \"predicted_rating\": predictions[np.isin(user_item_matrix.columns, unrated_items)]\n",
    "}).sort_values(by=\"predicted_rating\", ascending=False).head(10)\n",
    "\n",
    "# Map Movie IDs to Titles\n",
    "recommendations = recommendations.merge(movies, on=\"movieId\")\n",
    "print(\"\\nTop 10 Recommendations:\")\n",
    "print(recommendations[[\"title\", \"predicted_rating\"]])\n",
    "\n",
    "# Optional: Visualization\n",
    "plt.figure(figsize=(8, 5))\n",
    "recommendations.set_index(\"title\")[\"predicted_rating\"].plot(kind=\"barh\", color=\"skyblue\")\n",
    "plt.title(f\"Top Recommendations for User {target_user_id}\")\n",
    "plt.xlabel(\"Predicted Rating\")\n",
    "plt.ylabel(\"Movies\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97ef03-e98a-4138-89a5-fe61dc92fc42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
